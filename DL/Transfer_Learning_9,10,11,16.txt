import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

from keras.optimizers import Adam
from keras.losses import SparseCategoricalCrossentropy
from keras.metrics import Accuracy
from keras.applications.vgg16 import VGG16
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Image Data Generator
img_generator = ImageDataGenerator(
    brightness_range=(0.5,1),
    channel_shift_range=0.2,
    horizontal_flip=True,
    vertical_flip=True,
    rescale = 1./255,
    validation_split=0.3
)

# Data loading
root_dir = "E:\SEM_7\LP-4\Final_Practical\LP-IV-datasets\Object_Detection(Ass6)\caltech-101-img"

img_generator_flow_train = img_generator.flow_from_directory(
    directory = root_dir,
    target_size = (224, 224),
    batch_size = 32,
    shuffle =True,
    subset = "training")

img_generator_flow_valid = img_generator.flow_from_directory(
    directory = root_dir,
    target_size = (224, 224),
    batch_size = 32,
    shuffle = True,
    subset = "validation")

# Display some images
imgs, labels = next(iter(img_generator_flow_train))
for img, labels in zip(imgs, labels):
    plt.imshow(img)
    plt.show()

# Load pre-trained model
model_path = "E:/SEM_7/LP-4/Final_Practical/LP-IV-datasets/Object_Detection(Ass6)/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"

base_model = VGG16(input_shape=(224, 224, 3),
                   include_top = False, 
                   weights = model_path)

# Freeze parameters
base_model.trainable = False

# Add custom classifier
model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.MaxPooling2D(),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(102, activation='softmax')
])

# Model summary
model.summary()

# Train classifier layers
adam = Adam(0.001)
model.compile(optimizer='adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
model.fit(img_generator_flow_train, 
          validation_data=img_generator_flow_valid, 
          steps_per_epoch=5,
          epochs = 10)

# Plot accuracy
plt.plot(model.history.history['accuracy'], label='train_accuracy')
plt.plot(model.history.history['val_accuracy'],label='test_accuracy')
plt.legend()

# Fine-tune hyperparameters and unfreeze more layers
base_model.trainable = True
model.compile(loss='categorical_crossentropy',metrics=['categorical_accuracy'],optimizer='adam')
model.fit(img_generator_flow_train, 
          validation_data=img_generator_flow_valid, 
          steps_per_epoch=5, 
          epochs=10)


"""  This practical performs object detection using transfer learning with a Convolutional Neural Network (CNN). Specifically, it uses the VGG16 architecture pre-trained on a large dataset (ImageNet) as a feature extractor and customizes a classifier on top for the object detection task. Here’s a breakdown of the steps, followed by viva questions.

Detailed Overview
Step A: Load Pre-trained CNN Model
Image Data Generator: This generates augmented images for training and validation. Data augmentation techniques like brightness adjustments, channel shifting, horizontal/vertical flips, and rescaling are used to increase the dataset's diversity and help the model generalize better.
Dataset Loading: The dataset is split into training and validation sets using a 70-30 split, and images are resized to (224, 224), the input size expected by VGG16.
Step B: Freeze Parameters in Lower Convolutional Layers
Loading the Pre-trained Model: The VGG16 model, without its top layers (classification layers), is loaded. The weights are provided by the pre-trained model file (vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5).
Freezing Layers: All layers in the pre-trained VGG16 model are frozen, meaning the weights in these layers will not be updated during training. This step leverages the pre-trained feature extraction layers and reduces the number of parameters that need to be trained.
Step C: Add Custom Classifier
Custom Classification Layers:
MaxPooling2D: A pooling layer reduces the spatial dimensions of the feature maps.
Flatten: This layer flattens the pooled feature maps into a single vector.
Dense Layer: A fully connected layer with 102 units (representing the 102 object classes in the dataset) and softmax activation. The softmax layer outputs probabilities for each class, enabling the model to make predictions.
Step D: Train the Classifier Layers
Model Compilation: The model uses the Adam optimizer with a learning rate of 0.001, categorical_crossentropy loss function (suitable for multi-class classification), and accuracy as the evaluation metric.
Training: The model is trained for 10 epochs. Since only the custom classifier layers are trainable, this initial phase focuses on learning effective weights for object classification based on the extracted features from the VGG16 layers.
Step E: Fine-tune Hyperparameters and Unfreeze Layers
Unfreezing Layers: To allow fine-tuning, the entire base VGG16 model is unfrozen. This allows the model to learn additional representations that may be more specific to the current dataset.
Re-training: The model is recompiled with the same loss and optimizer settings. It is then trained for 10 additional epochs, allowing both the base model and classifier layers to learn from the training data.
Summary of the Practical
This implementation uses the VGG16 model as a feature extractor through transfer learning, adding a custom classifier to classify objects. By freezing and then selectively unfreezing layers, the model captures general features initially and then fine-tunes on the specific dataset. This approach is efficient for achieving good performance on a smaller dataset and reduces the computational cost compared to training from scratch.

Viva Questions and Suggested Answers
What is transfer learning?

Answer: Transfer learning is a technique where a model pre-trained on a large dataset is used as a starting point for a different, but related, task. It leverages previously learned features, reducing the amount of data and time needed for training.
Why is VGG16 a good choice for transfer learning?

Answer: VGG16 is a deep CNN architecture pre-trained on ImageNet, which is a large and diverse dataset. Its deep layers learn to capture complex patterns in images, which are useful for various computer vision tasks.
What is the significance of freezing layers in transfer learning?

Answer: Freezing layers preserves the pre-trained weights, preventing them from being updated. This saves computation and retains the general features learned from the larger dataset, which is beneficial for small target datasets.
Why do we use a Dense layer with softmax activation at the end?

Answer: The Dense layer with softmax activation produces probabilities for each class, which is required for multi-class classification. Softmax normalizes outputs into a probability distribution over all classes.
What is data augmentation, and why is it important?

Answer: Data augmentation artificially expands the dataset by applying random transformations, such as flips, rotations, and brightness changes. It helps improve model generalization by exposing it to varied versions of the training images.
How does the Adam optimizer work, and why is it used here?

Answer: Adam combines the advantages of two other optimizers, AdaGrad and RMSProp, by adapting the learning rate for each parameter. It’s widely used for its efficiency and fast convergence, making it suitable for training deep learning models.
What is fine-tuning, and why is it done after initial training?

Answer: Fine-tuning is the process of unfreezing some or all layers in a pre-trained model to allow further adjustment of weights. It allows the model to adapt more to the specific dataset by refining the learned features.
Why is categorical_crossentropy used as the loss function here?

Answer: Categorical_crossentropy is used for multi-class classification, measuring the difference between the predicted probability distribution and the true distribution (one-hot encoded labels) for each class.
What is the difference between ImageDataGenerator’s flow_from_directory and fit?

Answer: flow_from_directory generates batches of augmented images directly from a directory, which is ideal for large datasets that don’t fit into memory. fit, on the other hand, is used to train models on in-memory data.
Why do we use a pooling layer after the base model?

Answer: Pooling layers reduce the spatial dimensions of the feature maps, which helps decrease the number of parameters and computation. This also makes the model less prone to overfitting by focusing on the most important features.
Summary Explanation
This code implements object detection through transfer learning, using the pre-trained VGG16 CNN to extract features, followed by a custom classifier trained for the specific dataset. By initially freezing layers and then fine-tuning them, the model balances the benefits of general learned features and dataset-specific refinements.






"""