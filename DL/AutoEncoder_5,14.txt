import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from sklearn.preprocessing import MinMaxScaler,StandardScaler
from sklearn.model_selection import train_test_split

df=pd.read_csv('ecg_autoencoder_dataset.csv',header=None)
df.head()
df.shape
df.count().isnull()

feature=df.drop(140,axis=1)
feature.head()

target=df[140]
target.head()

scalar=StandardScaler()
featurescaled=scalar.fit_transform(feature)

xtrain,xtest,ytrain,ytest=train_test_split(featurescaled,target,test_size=0.2,random_state=40,stratify=target)

input1=xtrain.shape[1]
print(input1)
inputl=Input(shape=input1,)
encoder=Dense(64,activation='relu')(inputl)
encoder=Dense(32,activation='relu')(encoder)
encoder=Dense(16,activation='relu')(encoder)
encoder=Dense(8,activation='relu')(encoder)

decoder=Dense(16,activation='relu')(encoder)
decoder=Dense(32,activation='relu')(decoder)
decoder=Dense(64,activation='relu')(decoder)
decoder=Dense(input1,activation='sigmoid')(decoder)

autoencoder=Model(inputs=inputl, outputs=decoder)
autoencoder.compile(optimizer='adam',metrics=['accuracy'],loss='mse')
history=autoencoder.fit(xtrain,xtrain,epochs=20,batch_size=32,validation_data=(xtest,xtest),shuffle=True)

reconstruct=autoencoder.predict(xtest)
mse=np.mean(np.power(xtest-reconstruct,2),axis=1)

threshold=mse.mean()+3*mse.std()

anomaly=mse>threshold
print(np.sum(anomaly))



"""  This practical covers implementing anomaly detection using an Autoencoder model, specifically applied to a credit card dataset. Anomaly detection with Autoencoders is particularly useful in cases where anomalies are rare or unusual patterns that deviate significantly from normal patterns. Here’s an overview of each step in the code, along with potential viva questions and answers.

Practical Overview
Step A: Import Required Libraries
Libraries Imported: Libraries like pandas, numpy, and tensorflow provide data handling, numerical operations, and deep learning functionalities, respectively.
Matplotlib: Used for plotting results, which can help in visualizing reconstruction errors or threshold distributions.
Scikit-learn Modules: StandardScaler is used for scaling the data, and train_test_split is used for splitting the dataset.
Step B: Upload / Access the Dataset
Dataset Loading: The dataset is loaded into a DataFrame using pd.read_csv(). Each row represents a credit card transaction, and each column is a feature.
Checking Missing Values: The code checks for missing values using df.count().isnull().
Separating Features and Target: Here, feature represents transaction data (with 141 columns) while target (the last column) labels each transaction as normal or anomalous.
Step C: Encoder Network
Data Scaling: The StandardScaler scales the features to a standard range (mean 0 and standard deviation 1) for better performance in training.
Training and Testing Split: The dataset is split into training (80%) and testing (20%) subsets with stratification based on the target, which maintains the distribution of normal and anomalous samples across both sets.
Encoder Layers: The encoder is a series of Dense layers with decreasing dimensions, which compresses the input data into a smaller, latent representation. This smaller representation captures essential features of the data in a compact form.
Step D: Decoder Network
Decoder Layers: The decoder has Dense layers that mirror the encoder structure, progressively expanding the compressed data back to the original input size. The goal is for the decoder to reconstruct the input as closely as possible, ideally learning a pattern that matches the majority (normal transactions) and reconstructing them accurately.
Sigmoid Activation: The output layer uses a sigmoid activation, keeping the output values between 0 and 1 to match the scaled input range.
Step E: Compiling and Training the Autoencoder Model
Compilation: The model is compiled with adam as the optimizer, mse (mean squared error) as the loss function, and accuracy as an evaluation metric. MSE measures the reconstruction error, crucial for identifying anomalies.
Training: The model is trained by feeding in xtrain as both input and target, allowing the Autoencoder to learn to reconstruct normal data patterns. Validation data (xtest) helps monitor the model's performance on unseen data.
Anomaly Detection
Reconstruction Error Calculation: The model predicts xtest, and the mean squared error is computed between the original and reconstructed data points. This error (mse) represents how well the Autoencoder could reconstruct each transaction.
Threshold Determination: A threshold is defined as the mean of mse plus three times the standard deviation. Transactions with an MSE greater than this threshold are labeled as anomalies.
Counting Anomalies: The number of transactions classified as anomalies is printed.
Summary of the Practical
This practical demonstrates using an Autoencoder for anomaly detection in a credit card dataset. The Autoencoder compresses the input data, learns the dominant pattern, and reconstructs normal transactions with minimal error. Anomalies, which deviate from these patterns, yield a high reconstruction error, allowing them to be identified. This is especially useful for fraud detection, where fraudulent transactions may follow atypical patterns compared to normal transactions.

Viva Questions and Suggested Answers
What is an Autoencoder, and how does it work in anomaly detection?

Answer: An Autoencoder is a neural network used for unsupervised learning. It has two parts: an encoder that compresses the data into a lower-dimensional latent space and a decoder that reconstructs the original input. In anomaly detection, the Autoencoder learns to reconstruct normal data with minimal error. Anomalies have higher reconstruction errors and can be detected using a threshold.
Why is the StandardScaler used in this practical?

Answer: StandardScaler standardizes the input data to have a mean of 0 and a standard deviation of 1. This scaling is important for neural networks as it ensures consistent gradients and improves model convergence.
What is the role of the encoder and decoder in an Autoencoder?

Answer: The encoder compresses the input into a latent representation, reducing the dimensionality while preserving essential features. The decoder reconstructs the compressed representation back to the original input shape, aiming to recover the input as accurately as possible.
Why do we use the mean squared error (MSE) as the loss function in this case?

Answer: MSE measures the average squared differences between the original and reconstructed data points, effectively quantifying reconstruction error. Lower MSE indicates the Autoencoder is accurately reconstructing normal data, while high MSE helps identify anomalies.
How do we set the threshold for detecting anomalies?

Answer: The threshold is set as the mean MSE plus three times the standard deviation of the MSE values for the test data. Any data point with an MSE above this threshold is considered an anomaly.
What would you do if you found the Autoencoder misclassifying normal transactions as anomalies?

Answer: I would consider adjusting the threshold, further tuning the model (e.g., changing layer sizes, adding more layers), or experimenting with alternative anomaly detection techniques, like one-class SVM, to improve classification.
Why is anomaly detection important in credit card transactions?

Answer: Anomaly detection helps identify fraudulent transactions by detecting unusual patterns. Since fraudulent transactions differ from normal patterns, they can be isolated using machine learning models like Autoencoders, which identify deviations.
What might be the challenges of using an Autoencoder for anomaly detection?

Answer: Challenges include setting an accurate threshold, managing imbalanced data (as anomalies are rare), and avoiding overfitting. If the Autoencoder is too powerful, it may learn to reconstruct even anomalies well, reducing detection accuracy.
What is the purpose of using relu and sigmoid activations in the encoder and decoder?

Answer: relu in the encoder helps capture features by allowing only positive values, leading to efficient compression. sigmoid in the output layer bounds the output between 0 and 1, matching the normalized input range.
Can this model handle real-time data? Why or why not?

Answer: The model can handle real-time data if it’s pre-trained and deployed in a live environment where it receives transactions, reconstructs them, and checks reconstruction error. However, real-time deployment requires optimized performance, such as reduced latency and fast processing.
Summary Explanation
In this practical, we implement anomaly detection on a credit card dataset using an Autoencoder model. The Autoencoder learns to reconstruct normal patterns in the data. Anomalies, which don’t conform to these patterns, yield high reconstruction errors, allowing them to be flagged based on a threshold. This approach is valuable for tasks like fraud detection, where anomalies represent potentially fraudulent behavior in a dataset of otherwise normal transactions. """